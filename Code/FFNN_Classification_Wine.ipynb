{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da2abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np  \n",
    "from autograd import grad, elementwise_grad\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c07ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some activation functions and their derivative\n",
    "def ReLU(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "def ReLU_der(z):\n",
    "    return np.where(z > 0, 1, 0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_der(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def mse(predict, target):\n",
    "    return np.mean((predict - target) ** 2)\n",
    "\n",
    "def mse_der(predict, target):\n",
    "    return 2 * (predict - target) / np.prod(predict.shape)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Compute softmax values for each set of scores in the rows of the matrix z.\n",
    "    Used with batched input data.\"\"\"\n",
    "    e_z = np.exp(z - np.max(z, axis=1)[:, np.newaxis]) #substract max per row, avoids instability\n",
    "    return e_z / np.sum(e_z, axis=1)[:, np.newaxis]\n",
    "\n",
    "def softmax_vec(z):\n",
    "    \"\"\"Compute softmax values for each set of scores in the vector z.\n",
    "    Use this function when you use the activation function on one vector at a time\"\"\"\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / np.sum(e_z)\n",
    "\n",
    "def softmax_der(z):\n",
    "    return np.ones_like(z) \n",
    "#Purely placeholder, combined derivative \n",
    "#Cross entropy + softmax simplifies to predict-target\n",
    "\n",
    "def cross_entropy(predict, target):\n",
    "    return np.mean(-np.sum(target * np.log(predict + 1e-10), axis=1))\n",
    "\n",
    "def cross_entropy_der(predict, target):\n",
    "    return (predict - target) / predict.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0be2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing creation of n layers for batched input shapes\n",
    "def create_layers_batch(network_input_size, layer_output_sizes):\n",
    "    layers = []\n",
    "\n",
    "    i_size = network_input_size\n",
    "    for layer_output_size in layer_output_sizes:\n",
    "        std = np.sqrt(2.0 / i_size)\n",
    "        W = np.random.randn(layer_output_size, i_size) * std\n",
    "        W = W.T\n",
    "        b = np.random.randn(layer_output_size)\n",
    "        layers.append((W, b))\n",
    "\n",
    "        i_size = layer_output_size\n",
    "    return layers\n",
    "\n",
    "#Applying weights, bias and activation function and passing forward\n",
    "def feed_forward_saver_batch(input, layers, activation_funcs):\n",
    "    layer_inputs = []\n",
    "    zs = []\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        layer_inputs.append(a)\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "\n",
    "        zs.append(z)\n",
    "\n",
    "    return layer_inputs, zs, a\n",
    "\n",
    "#Same, but when saving layer_inputs and zs is not needed\n",
    "def feed_forward(input, layers, activation_funcs):\n",
    "    a = input\n",
    "    for (W, b), activation_func in zip(layers, activation_funcs):\n",
    "        z = a @ W + b\n",
    "        a = activation_func(z)\n",
    "    return a\n",
    "\n",
    "#Verify gradients\n",
    "def cost(input, layers, activation_funcs, target):\n",
    "    predict = feed_forward_saver_batch(input, layers, activation_funcs)[2]\n",
    "    return mse(predict, target)\n",
    "\n",
    "#Computing gradients\n",
    "def backpropagation_batch(\n",
    "    input, layers, activation_funcs, target, activation_ders, cost_der=mse_der\n",
    "):\n",
    "    layer_inputs, zs, predict = feed_forward_saver_batch(input, layers, activation_funcs)\n",
    "\n",
    "    layer_grads = [() for layer in layers]\n",
    "\n",
    "    # We loop over the layers, from the last to the first\n",
    "    for i in reversed(range(len(layers))):\n",
    "        layer_input, z, activation_der = layer_inputs[i], zs[i], activation_ders[i]\n",
    "\n",
    "        if i == len(layers) - 1:\n",
    "            # For last layer we use cost derivative as dC_da(L) can be computed directly\n",
    "            dC_da = cost_der(predict, target)\n",
    "        else:\n",
    "            # For other layers we build on previous z derivative, as dC_da(i) = dC_dz(i+1) * dz(i+1)_da(i)\n",
    "            (W, b) = layers[i + 1]\n",
    "            dC_da = dC_dz @ W.T\n",
    "\n",
    "        dC_dz = dC_da * activation_der(z)\n",
    "        dC_dW = layer_input.T @ dC_dz  #W gradients for batches\n",
    "        dC_db = np.sum(dC_dz, axis=0)   #sum bias gradients, batch dim\n",
    "\n",
    "        layer_grads[i] = (dC_dW, dC_db)\n",
    "\n",
    "    return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9752b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.43607120e-02  0.00000000e+00 -5.65599844e-05  1.25980236e-01]\n",
      "[-5.43607120e-02  0.00000000e+00 -5.65599844e-05  1.25980236e-01]\n",
      "Same gradients\n"
     ]
    }
   ],
   "source": [
    "#Gradient verification with autograd\n",
    "\n",
    "network_input_size = 4\n",
    "batch_size = 400\n",
    "layer_output_sizes = [3, 4]\n",
    "activation_funcs = [sigmoid, ReLU]\n",
    "activation_ders = [sigmoid_der, ReLU_der]\n",
    "la = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "x = np.random.randn(batch_size, network_input_size)\n",
    "input = x\n",
    "target = np.random.rand(4)\n",
    "\n",
    "computed = backpropagation_batch(input, la, activation_funcs, target, activation_ders)\n",
    "print(computed[-1][0][0])\n",
    "\n",
    "cost_grad = grad(cost, 1)\n",
    "autoG = cost_grad(input, la, activation_funcs, target)\n",
    "print(autoG[-1][0][0])\n",
    "\n",
    "diff = 0\n",
    "for i in range(len(computed)):\n",
    "    diff += abs(computed[-1][0][i][0])-abs(autoG[-1][0][0][0])\n",
    "if diff <= 10**-6:\n",
    "    print(\"Same gradients\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c30b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6. 6. ... 6. 7. 6.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Applying FFNN base above to classification problem, MNIST:\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Fetch the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "\n",
    "#Extract data\n",
    "X = mnist.data\n",
    "y_labels = mnist.target.astype(int)\n",
    "X = X / 255.0\n",
    " \n",
    "\n",
    "#One-hot encode labels\n",
    "y = np.eye(10)[y_labels] #Convert to integers\n",
    "\n",
    "#Split into train and test\n",
    "#y_train_labels and y_test_labels for scikit-learn, expects integer labels\n",
    "X_train, X_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split(\n",
    "    X, y, y_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "X = data.drop('quality', axis=1).values\n",
    "y_reg = data['quality'].values.astype(float)           # regression target\n",
    "y_cls = data['quality'].values - 3                     # classes 0–6 for classification\n",
    "\n",
    "# Optional: turn into binary classification (good ≥7, bad ≤5, neutral=6→bad or remove)\n",
    "# y_binary = (data['quality'] >= 7).astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, yreg_train, yreg_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "#X_train_cls, X_test_cls, ycls_train, ycls_test = train_test_split(X, y_cls, test_size=0.2, random_state=42)\n",
    "\n",
    "print(y_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f532615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    one_hot_predictions = np.zeros(predictions.shape)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        one_hot_predictions[i, np.argmax(prediction)] = 1\n",
    "    return accuracy_score(one_hot_predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c03a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# OPTIMIZER FACTORY + STATE\n",
    "# ============================\n",
    "\n",
    "def create_optimizer(optimizer_name=\"adam\", lr=0.001):\n",
    "    if optimizer_name == \"sgd\":\n",
    "        return {\"name\": \"sgd\", \"lr\": lr}\n",
    "    \n",
    "    elif optimizer_name == \"adam\":\n",
    "        return {\n",
    "            \"name\": \"adam\",\n",
    "            \"lr\": lr,\n",
    "            \"beta1\": 0.9,\n",
    "            \"beta2\": 0.999,\n",
    "            \"epsilon\": 1e-8,\n",
    "            \"t\": 0,\n",
    "            \"m\": [],   # will hold (m_W, m_b) for each layer\n",
    "            \"v\": []    # will hold (v_W, v_b) for each layer\n",
    "        }\n",
    "    \n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        return {\n",
    "            \"name\": \"rmsprop\",\n",
    "            \"lr\": lr,\n",
    "            \"beta\": 0.99,\n",
    "            \"epsilon\": 1e-8,\n",
    "            \"v\": []\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "# ============================\n",
    "# MAIN UPDATE FUNCTION\n",
    "# ============================\n",
    "\n",
    "def update_parameters(layers, grads, optimizer_config, t=None):\n",
    "    \"\"\"\n",
    "    One function to rule them all.\n",
    "    Pass in your layers, gradients, and optimizer config → returns updated layers\n",
    "    \"\"\"\n",
    "    name = optimizer_config[\"name\"]\n",
    "    \n",
    "    if name == \"sgd\":\n",
    "        lr = optimizer_config[\"lr\"]\n",
    "        updated_layers = []\n",
    "        for (W, b), (dW, db) in zip(layers, grads):\n",
    "            W = W - lr * dW\n",
    "            b = b - lr * db\n",
    "            updated_layers.append((W, b))\n",
    "        return updated_layers\n",
    "\n",
    "    elif name == \"adam\":\n",
    "        # Initialize m and v on first call\n",
    "        if len(optimizer_config[\"m\"]) == 0:\n",
    "            optimizer_config[\"m\"] = [(np.zeros_like(W), np.zeros_like(b)) for W, b in layers]\n",
    "            optimizer_config[\"v\"] = [(np.zeros_like(W), np.zeros_like(b)) for W, b in layers]\n",
    "        \n",
    "        beta1 = optimizer_config[\"beta1\"]\n",
    "        beta2 = optimizer_config[\"beta2\"]\n",
    "        lr = optimizer_config[\"lr\"]\n",
    "        eps = optimizer_config[\"epsilon\"]\n",
    "        t = optimizer_config[\"t\"] + 1\n",
    "        optimizer_config[\"t\"] = t  # update timestep\n",
    "\n",
    "        updated_layers = []\n",
    "        for i, ((W, b), (dW, db)) in enumerate(zip(layers, grads)):\n",
    "            m_W, m_b = optimizer_config[\"m\"][i]\n",
    "            v_W, v_b = optimizer_config[\"v\"][i]\n",
    "\n",
    "            # Adam update (per parameter)\n",
    "            m_W = beta1 * m_W + (1 - beta1) * dW\n",
    "            m_b = beta1 * m_b + (1 - beta1) * db\n",
    "            v_W = beta2 * v_W + (1 - beta2) * (dW ** 2)\n",
    "            v_b = beta2 * v_b + (1 - beta2) * (db ** 2)\n",
    "\n",
    "            m_hat_W = m_W / (1 - beta1 ** t)\n",
    "            m_hat_b = m_b / (1 - beta1 ** t)\n",
    "            v_hat_W = v_W / (1 - beta2 ** t)\n",
    "            v_hat_b = v_b / (1 - beta2 ** t)\n",
    "\n",
    "            W = W - lr * m_hat_W / (np.sqrt(v_hat_W) + eps)\n",
    "            b = b - lr * m_hat_b / (np.sqrt(v_hat_b) + eps)\n",
    "\n",
    "            # Save back\n",
    "            optimizer_config[\"m\"][i] = (m_W, m_b)\n",
    "            optimizer_config[\"v\"][i] = (v_W, v_b)\n",
    "            updated_layers.append((W, b))\n",
    "\n",
    "        return updated_layers\n",
    "\n",
    "    # You can easily add RMSprop, AdamW, etc. here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff7ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, keep_prob=0.8, training=True):\n",
    "    \"\"\"\n",
    "    keep_prob = probability a neuron is KEPT (not dropped)\n",
    "    0.8 → drop 20 % of neurons (best for hidden layers)\n",
    "    0.5 → drop 50 % (sometimes used, but 0.8–0.9 is better for ReLU nets)\n",
    "    \"\"\"\n",
    "    if training:\n",
    "        mask = np.random.rand(*x.shape) < keep_prob     # random mask\n",
    "        return x * mask / keep_prob                     # ← INVERTED DROPOUT (important!)\n",
    "    else:\n",
    "        return x                                        # no dropout at test time\n",
    "    \n",
    "def ReLU_with_dropout(z, keep_prob=0.8, training=True):\n",
    "    a = ReLU(z)\n",
    "    return dropout(a, keep_prob=keep_prob, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "910ca793",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Create layers\u001b[39;00m\n\u001b[0;32m     62\u001b[0m layers \u001b[38;5;241m=\u001b[39m create_layers_batch(network_input_size, layer_output_sizes)\n\u001b[0;32m     64\u001b[0m trained_layers, losses, train_predictions, train_accs, test_accs \u001b[38;5;241m=\u001b[39m train_network(\n\u001b[1;32m---> 65\u001b[0m     X_train, y_train, layers, activation_funcs, activation_ders, learning_rate, epochs, batch_size\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Plot loss as function of epoch\u001b[39;00m\n\u001b[0;32m     69\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_network(X_train, y_train, layers, activation_funcs, activation_ders, learning_rate, epochs, batch_size=100):\n",
    "    losses = []\n",
    "    train_accs = []  \n",
    "    test_accs = [] \n",
    "    #\n",
    "    optimizer = create_optimizer(\"adam\", lr=0.001)   # or \"sgd\", lr=0.05\n",
    "    #\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle digits, avoids learning misleading patterns. Independent data\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled = X_train[perm] \n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        # Mini-batch loop\n",
    "        for start in range(0, X_train.shape[0], batch_size):\n",
    "            end = start + batch_size\n",
    "            input_batch = X_shuffled[start:end]\n",
    "            target_batch = y_shuffled[start:end]\n",
    "            \n",
    "            layers_grad = backpropagation_batch(input_batch, layers, activation_funcs, target_batch, activation_ders, cost_der=cross_entropy_der)\n",
    "            #\n",
    "            layers = update_parameters(layers, layers_grad, optimizer)\n",
    "            #\n",
    "            for (W, b), (W_g, b_g) in zip(layers, layers_grad):\n",
    "                W -= learning_rate * W_g\n",
    "                b -= learning_rate * b_g\n",
    "        \n",
    "        # Compute loss after epoch\n",
    "        train_predictions = feed_forward(X_train, layers, activation_funcs)\n",
    "        test_predictions = feed_forward(X_test, layers, activation_funcs)\n",
    "        loss = cross_entropy(train_predictions, y_train)\n",
    "        TESTacc = accuracy(test_predictions, y_test)\n",
    "        TRAINacc = accuracy(train_predictions, y_train)\n",
    "        losses.append(loss)\n",
    "        train_accs.append(TRAINacc)  \n",
    "        test_accs.append(TESTacc)    \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss:.3f}, Test Accuracy: {TESTacc:.3f}, Train Accuracy: {TRAINacc:.3f}\") \n",
    "    \n",
    "    return layers, losses, train_predictions, train_accs, test_accs  # Updated return\n",
    "\n",
    "# Setup network skeleton \n",
    "network_input_size = 784\n",
    "layer_output_sizes = [512, 256, 128, 10]  # output 10 classes\n",
    "#activation_funcs = [ReLU, ReLU, softmax]\n",
    "activation_ders = [ReLU_der, ReLU_der, ReLU_der, softmax_der]\n",
    "\n",
    "#NEW\n",
    "# After (dropout on hidden layers only!)\n",
    "activation_funcs = [\n",
    "    lambda z: ReLU_with_dropout(z, keep_prob=0.99, training=True),\n",
    "    lambda z: ReLU_with_dropout(z, keep_prob=0.99, training=True),\n",
    "    lambda z: ReLU_with_dropout(z, keep_prob=0.99, training=True),\n",
    "    softmax\n",
    "]\n",
    "\n",
    "# Train\n",
    "epochs = 50  \n",
    "learning_rate = 0.05\n",
    "batch_size = 100\n",
    "\n",
    "# Create layers\n",
    "layers = create_layers_batch(network_input_size, layer_output_sizes)\n",
    "\n",
    "trained_layers, losses, train_predictions, train_accs, test_accs = train_network(\n",
    "    X_train, y_train, layers, activation_funcs, activation_ders, learning_rate, epochs, batch_size\n",
    ")\n",
    "\n",
    "# Plot loss as function of epoch\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), losses, label='Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Loss vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracies (train and test) as function of epoch\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_accs, label='Train Accuracy', marker='o', linestyle='-')\n",
    "plt.plot(range(1, epochs + 1), test_accs, label='Test Accuracy', marker='x', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "#Scikit-learn Logistic Regression comparison\n",
    "model = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10, random_state=42)\n",
    "model.fit(X_train, y_train_labels)\n",
    "y_pred = model.predict(X_test)\n",
    "sk_accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "print(f\"Scikit-learn Model Accuracy: {sk_accuracy:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b26a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlearning_rate = 0.02\\nEpoch 148, Loss: 0.149, Test Accuracy: 0.930, Train Accuracy: 0.956\\nEpoch 149, Loss: 0.155, Test Accuracy: 0.928, Train Accuracy: 0.954\\nEpoch 150, Loss: 0.151, Test Accuracy: 0.927, Train Accuracy: 0.956\\n\\nDiscuss your results and give a critical analysis of the various parameters, including hyper-parameters like the learning rates and the regularization parameter \\n, various activation functions, number of hidden layers and nodes and activation functions\\n'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "learning_rate = 0.02\n",
    "Epoch 148, Loss: 0.149, Test Accuracy: 0.930, Train Accuracy: 0.956\n",
    "Epoch 149, Loss: 0.155, Test Accuracy: 0.928, Train Accuracy: 0.954\n",
    "Epoch 150, Loss: 0.151, Test Accuracy: 0.927, Train Accuracy: 0.956\n",
    "\n",
    "Discuss your results and give a critical analysis of the various parameters, including hyper-parameters like the learning rates and the regularization parameter \n",
    ", various activation functions, number of hidden layers and nodes and activation functions\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc8b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# (W, b)  new params\\n# (W_g, b_g) gradients of old params \\n\\n\\n\\n layers_grad = backpropagation_batch(input_batch, layers, activation_funcs, target_batch, activation_ders, cost_der=cross_entropy_der)\\n            for (W, b), (W_g, b_g) in zip(layers, layers_grad):\\n                W -= learning_rate * W_g\\n                b -= learning_rate * b_g\\n\\n\\n\\n layers_grad = backpropagation_batch(input_batch, layers, activation_funcs, target_batch, activation_ders, cost_der=cross_entropy_der)\\n            for (W, b), (W_g, b_g) in zip(layers, layers_grad):\\n                W -= ADAM(W_g)\\n                b -= ADAM(b_g)\\n\\n\\ndef ADAM(gradient):\\n   \\n   beta1 = 0.9            #dacay rate nr 1\\n   beta2 = 0.999          #decay rate nr 2\\n   alpha = 0.001          #learning rate  \\n   epsilon = 10**-8 \\n\\n   #init values\\n   m_tprev = 0         #m for previous time step \\n   v_tprev = 0\\n   t = 1               #current t, starting counting at 1    \\n   \\n   m_t = beta1 * (m_tprev) + (1-beta1) * gradient\\n   v_t = beta2 * v_tprev + (1-beta2)*(gradient**2)\\n\\n   m_hat = m_t / (1-beta1**t)\\n   v_hat = v_t / (1-beta2**t)\\n\\n   return alpha * m_hat/(np.sqrt(v_hat)+epsilon)\\n\\n   \\n\\n\\n\\n#normal GD\\nW -= learning_rate * W_g\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# (W, b)  new params\n",
    "# (W_g, b_g) gradients of old params \n",
    "\n",
    "\n",
    "\n",
    " layers_grad = backpropagation_batch(input_batch, layers, activation_funcs, target_batch, activation_ders, cost_der=cross_entropy_der)\n",
    "            for (W, b), (W_g, b_g) in zip(layers, layers_grad):\n",
    "                W -= learning_rate * W_g\n",
    "                b -= learning_rate * b_g\n",
    "\n",
    "\n",
    "\n",
    " layers_grad = backpropagation_batch(input_batch, layers, activation_funcs, target_batch, activation_ders, cost_der=cross_entropy_der)\n",
    "            for (W, b), (W_g, b_g) in zip(layers, layers_grad):\n",
    "                W -= ADAM(W_g)\n",
    "                b -= ADAM(b_g)\n",
    "\n",
    "\n",
    "def ADAM(gradient):\n",
    "   \n",
    "   beta1 = 0.9            #dacay rate nr 1\n",
    "   beta2 = 0.999          #decay rate nr 2\n",
    "   alpha = 0.001          #learning rate  \n",
    "   epsilon = 10**-8 \n",
    "\n",
    "   #init values\n",
    "   m_tprev = 0         #m for previous time step \n",
    "   v_tprev = 0\n",
    "   t = 1               #current t, starting counting at 1    \n",
    "   \n",
    "   m_t = beta1 * (m_tprev) + (1-beta1) * gradient\n",
    "   v_t = beta2 * v_tprev + (1-beta2)*(gradient**2)\n",
    "\n",
    "   m_hat = m_t / (1-beta1**t)\n",
    "   v_hat = v_t / (1-beta2**t)\n",
    "\n",
    "   return alpha * m_hat/(np.sqrt(v_hat)+epsilon)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#normal GD\n",
    "W -= learning_rate * W_g\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
